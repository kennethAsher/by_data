#####测试#####
input_path = hdfs:///test/spark/input
input_file = hdfs:///hive/imp/organ_add_data/
output_path = hdfs:///test/pyspark/demo/output6/



#####清洗#####路径
#原始裁判文书位置
organ_data_path = hdfs:///hive/imp/organ_data/
#原始补充数据位置
organ_add_data_path = hdfs:///hive/imp/organ_add_data/
#补充数据完成补充之后的路径
add_data_path = hdfs:///hive/imp/add_data/
#清洗案由，当事人的数据存放路径
case_party_path = hdfs:///hive/imp/case_party_data/
#清洗出得律师审判人员数据
lawyer_judge_path = hdfs:///hive/imp/lawyer_judge_data



#####工具#####路径
#级别案由原文件存放位置
cause_of_action_path = hdfs:///hive/by/pg_data/pg_sm_cause_of_action/pg_sm_cause_of_action.txt
#百家姓文件存放的位置
family_file_path = hdfs:///hive/by/pg_data/pg_party_type/family_names.txt
#有标签的企业文件存放的位置
label_company_file_path = hdfs:///hive/by/pg_data/pg_party_type/labels_firm.txt


#####配置名称
#Spark SupplementAddData的appName
supplement_add_data_appname = SupplementData
#清洗castparty文书的appName
Case_party_appname = CaseParty
#清洗律师审判人员信息的appName
lawyer_judge_appname = LawyerJudge